## 18.03
* Сделал препроцессирование данных (формирование словарей, выделенение out of vocabulary слов и пр.).
* Разобрался со схемой работы сверточного слоя.
* Наметил общую структуру baseline модели, реализовал инициализацию слоев и весов.
* Реализовал потенциальное решение проблемы OOV слов (но скорее всего откажусь от него в пользу случайной инициализации и дообучения).

## 22.03
* Завершил общую структуру baseline модели, реализовал циклы обучения и оценки.
* Убедился в работоспособности модели, обучив ее на 1000 примерах из train датасета.
  
  ![Alt text](old_plots/mini_full.png?raw=true "Title")

Полностью модель выучить примеры не смогла, достигнув F1 порядка 0.99 на 80ой эпохе, что может быть плохим знаком.

* Далее попробовал обучить модель на полном датасете и получил результаты, которые затрудняюсь объяснить:
  
  ![Alt text](old_plots/train2.png?raw=true "Title")
  ![Alt text](old_plots/valid2.png?raw=true "Title")
  
  Модель достигает на первой эпохе F1 меры 0.83 и перестает обучаться. Сначала я предположил, что проблема в слишком маленьком lr или в слишком сильной регуляризации dropout'ом, но эксперименты с увеличением первого и уменьшением второго только ухудшили результаты. Затем я предположил, что модель полностью обучилась к концу первой эпохи, а переобучаться ей мешал сильный dropout. Но в таком случае нужна модель много меньшей мощности, из чего я выдвинул предположение, что какие-то слои почему-то не работают. По аналогии с FFNN без добавления нелинейных активаций. Это бы объяснило, почему модель так и не смогла выучить 1000 примеров.
  
  Тогда я убрал сначала CNN слой и обучил оставшуюся модель на тех же 1000 примерах из train выборки:
  
 ![Alt text](old_plots/mini_cnn.png?raw=true "Title")
 
 А затем попробовал убрать bLTSM слой:
 
 ![Alt text](old_plots/mini_lstm.png?raw=true "Title")
 
 И вообще убрать все, кроме CRF слоя и эмбеддингов слов:
 
 ![Alt text](old_plots/mini_crf.png?raw=true "Title")
 
 Конечные результаты:
 
| Модель      | F1     |
| ------      | ------ |
| CNNbLSTMCRF | 0.9887 |
| bLSTMCRF    | 0.9867 |
| CNNCRF      | 0.9091 |
| CRF         | 0.9265 |

Не уверен, что из этого можно делать какие-либо выводы, но факт того, что при добавлении к CRF CNN слоя F1 мера падает почти на 0.02, настораживает. Возможно, проблема в нем.

## 25.03
* Поменял метрику оценки на не учитывающую значение F1 меры для класса 'O'.
  
  Значения новой F1 меры и функции потерь в процессе обучения:
  
  ![Alt text](old_plots/train_nm.png?raw=true "Title")
  
  Как и ожидалось, модель за первую эпоху учится всегда ставить метку 'O' (recall этого класса равен 1.0). Аналогичное плато можно видеть и при обучении на меньших датасетах:
  
  1000 примеров
  
  ![Alt text](old_plots/mini_high_lr_high_decay.png?raw=true "Title")
  
  5000 примеров
  
  ![Alt text](old_plots/mini_5k_a_lr005.png?raw=true "Title")
  
  В ходе экспериментов с меньшими датасетами установил, что размер плато зависит как от размера выборки, так и от гиперпараметров, влияющих на learning rate. Возможно, существует такая комбинация гиперпараметров, при которой модель выйдет из плато в разумные сроки.
  
  ## 29.03
  * Поменял метрику на оценивающую правильность определения именованных сущностей, а не выбора класса метки.
  * Изменил процесс разбиение по батчам, чтобы модель обучалась на цельных предложениях.
  * С помощью раннего останова добился качества, сравнимого с заявленным в статье:

  Значение функции потерь на обучающем и валидационном наборах после 5-ти эпох обучения:
  
  ![Alt text](plots/loss_5_best.png?raw=true "Title")
  
  F1 мера на обучающем и валидационном наборах после 5-ти эпох обучения:
  
  ![Alt text](plots/f1_5_best.png?raw=true "Title")
  
  Конечные результаты:
  
  | Набор | F1     |
  | ----- | ------ |
  | Train | 0.9962 |
  | Valid | 0.9474 |
  | Test  | 0.9092 |
